(1.1) :  ORCA_CRITICAL_X is failing on FDSC only where it appears downloading requests, launching online sessions, etc. 
(1.3) :  I'm not able to launch online after logging into FDSC05 and unable to run fqlc/usc command line requests on FDSC01 as well. 
(2.2) :  But all nodes that I have tried on FDSC (C01-C05) have the same problem. 
(3.1) :  The issue seems to be with fqlc. 
(3.2) :  I have tried a few different formulas and they all hang after returning the result: fqlc ibm "price()" fqlc 7535 "teca_dps_act(0)" fqlc 635688 "TECI_SALES_FY1R()" fqlf works fine. 
(4.1) :  USC hangs as well. 
(4.2) :  Also unable to launch a gui online session. 
(8.1) :  Looking into this with Ed and Pavel. 
(8.3) :  fdsa7, fdsa8 is having the same hanging issue, but other nodes seems fine. 
(9.1) :  The problem may be AUDs. 
(9.2) :  I've disabled the auds shadow mode on all FDSC nodes and FDSA07 and FDSA08. 
(9.3) :  Online started on FDSC08 after I disabled auds. 
(10.1) :  That seems to have fixed the issue on fdsc05. 
(11.1) :  Bumping this up to Critical as this is affecting Online on at least FDSA07. 
(11.2) :  Run gui_online hangs there. 
(11.3) :  Although not an interactive node, there's production batch queues that this might affect. 
(15.1) :  From prior connection to FDSA07, i'm getting in usage log: TEXT:AUDS:Session:Transport Exception: ECONNRESET The connection is timeing out at it goes offbox to Fonix? 
(16.1) :  Confirmed all the ORCA Critical jobs are now looking good. 
(16.2) :  I aborted the hung jobs. 
(18.1) :  Trying to assess the impact of this to productions/clients, as I'm not too familiar with AUDS and it's interactions with batch queues in FDSA07 and FDSA08 A things questions I have in my mind: 1 - I can understand why this affected online (from reading TLB files offbox). 
(18.2) :  But this also hanged ORCA and basic FQLF, Would this affect client Cornerstone jobs that run on these nodes? 
(18.3) :  2 - Would this impact production jobs that tries to evaluate an FQL code (eg Price or Symbology codes)? 
(19.2) :  unless the production job is specifically submitted to run on fdsa07/8 (except express - and I'm having a discussion with vms team about that), it will not land on these nodes. 
(19.3) :  Jobs submitted to sys$batch (which is the default batch queue) will not execute on fdsa07/8. 
(20.1) :  This stemmed a disk issue on the audsdev nodes causing our logging process to be killed, this increased the memory footprint of our processes and cause them to start failing over time. 
(20.2) :  I was paged about the issue once it became critical on the auds side. 
(20.4) :  Based on the logs it looks like requests went through their failure limits for the local site and then jumped to the B site. 
(20.5) :  There is a long initial session establishment timeout that can cause a process to block while processing the event in the shadow queue, even if the process is exiting. 
(20.7) :  The auds servers themselves don't really need much disk space except to temporarily store log data in flume, the server with the issue is doing double duty as our management (puppet, graphite, dashboard, builds) server. 
(21.1) :  Just noting here that no Cornerstone jobs have run on fdsa07/08. 
(22.2) :  I've deployed the auds server processes to a cassandra node which is a physical box with much more memory and disk space than the audsdev VM. 
(22.3) :  I'll continue doing this going forward until we have production hardware. 
(22.4) :  Tomorrow I'll recreate the client-side issue and look at failing faster in shadow mode. 
(22.5) :  @GregJordan please re-enable auds shadow mode on the test nodes. 
(23.1) :  @MarkGuzman shadow mode is still enabled on the FDSD nodes. 
(23.2) :  Before re-enableing elsewhere, I would like to better understand the failure scenarios and the A to B site failover. 
(24.1) :  Sure, in this case the client will try to connect to a box 3 times before failing the request. 
(24.2) :  It will prefer the same site as the client but will fail to other available servers if there are no "local" servers available. 
(24.3) :  Currently we don't remove the failing server from the pool, so subsequent requests will go through the same process if we are in a "zombie" state on the server-side. 
(24.4) :  We expect that a failing server will either die or be killed by monit (due to memory use or cpu use). 
(24.5) :  A dead server is automatically removed from the client's list of servers via zookeeper callback. 
(24.6) :  Session establishment is a special case and tries a bit harder to succeed, as all requests to private/secured content will fail without valid session credentials. 
(24.7) :  All of these requests get put into the shadow queue to be processes by the shadow thread. 
(24.8) :  The shadow thread will not attempt to process all requests in it's cue at shutdown, this is done to prevent it from delaying existing jobs. 
(24.10) :  The shutdown code will block waiting for the shadow thread to complete it's current task. 
(24.12) :  ï»¿It's worth noting that if there are no servers available in the client's pool and zookeeper support is enabled then it will attempt to fetch servers from zookeeper 10 times if it needs to fulfil a request. 
(25.1) :  @DemetryZilberg , @GregJordan , @SeanBattis and I just met regarding our failure scenarios and site failover. 
(25.2) :  The primary concern is that shadow mode can cause a long delay if there is a failure on the server side. 
(25.3) :  The client itself has retries, redundancy and failover but those actually help to cause this problem. 
(25.4) :  We want shadow mode to have no impact to the end user even if it is facing a major failure. 
(25.6) :  Remove pool members that have reported errors from a client's connection pool for some period of time. 
(25.10) :  Migrate our monitoring/dashboard to audsadma01 when it's been rebuilt by unix team. 
(25.11) :  Demetry will be working with @JerryUanino to get us a physical audsdev box. 
(25.12) :  This is expected to take a couple of weeks and is seen as a prerequisite for client shadow mode testing. 
(25.14) :  Once the failure mode testing is complete we will be resuming fdsa devel/inhouse testing. 
